{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c73146fb-8a09-4087-bad6-d37433e19ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import operator\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score,accuracy_score\n",
    "from matplotlib.pyplot import plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "342dd54d-5414-4176-9206-ed1c5f92a3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data from txt\n",
    "def read_data(set_name):\n",
    "    text_file_name  = set_name + \"_text.txt\"\n",
    "    label_file_name = set_name + \"_labels.txt\"\n",
    "    text_file = open(\"data/\" + text_file_name, \"r\", encoding=\"utf8\")\n",
    "    label_file = open(\"data/\" + label_file_name, \"r\", encoding=\"utf8\")\n",
    "    x = text_file.readlines()\n",
    "    y = label_file.readlines()\n",
    "    for i in range(len(y)): y[i] = y[i][0]\n",
    "    return pd.DataFrame(x, columns=[\"text\"]), pd.DataFrame(y, columns=[\"label\"])\n",
    "train_set_x, train_set_y = read_data(\"train\")\n",
    "val_set_x,   val_set_y   = read_data(\"val\")\n",
    "test_set_x,  test_set_y  = read_data(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c01cf78a-f6c2-4a43-af12-fb3230a992c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vector(vocab, text):\n",
    "    vector = np.zeros(len(vocab))\n",
    "    words = []\n",
    "    for sentence in nltk.tokenize.sent_tokenize(text):\n",
    "        for token in nltk.tokenize.word_tokenize(sentence):\n",
    "            words.append(lemmatizer.lemmatize(token).lower())\n",
    "    for i, word in enumerate(vocab):\n",
    "        if word in words:\n",
    "            vector[i] = words.count(word)\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5a065354-b63b-49bb-8cdc-0d8359d71ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label mapping declaration\n",
    "SENTIMENTAL_MAP = {\n",
    "    \"0\": \"negative\",\n",
    "    \"1\": \"neutral\",\n",
    "    \"2\": \"positive\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ffa6a940-d86a-4373-8b4e-ebb3c6e28d73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# Initial Feature: n most frequent words of each label class and combining them together\n",
    "# define stopwords\n",
    "stopwords = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "additional_stopwords = [\".\", \",\", \"'s\", \"``\", \"''\", \"'\", \"n't\", \"%\", \"-\", \"$\", \"(\", \")\", \":\", \";\", \"@\", \"&\", \"'m\", \"user\", \"#\", \"!\", \"?\", \"...\"]\n",
    "for sw in additional_stopwords: stopwords.add(sw)\n",
    "\n",
    "# Initialize lemmatizer\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "# Get Vocabulary\n",
    "vocabulary = []\n",
    "n = 100\n",
    "for label in SENTIMENTAL_MAP.keys():\n",
    "    print(train_set_y.loc[1, \"label\"])\n",
    "    # get texts with same label\n",
    "    temp_list = []\n",
    "    for i in train_set_x.index:\n",
    "        if train_set_y.loc[i, \"label\"] == label:\n",
    "            temp_list.append(train_set_x.loc[i, \"text\"])\n",
    "    \n",
    "    # get n most frequent words of this label class\n",
    "    dict_word_freq = {}\n",
    "    for text in temp_list:\n",
    "        for sentence in nltk.tokenize.sent_tokenize(text):\n",
    "            for token in nltk.tokenize.word_tokenize(sentence):\n",
    "                word = lemmatizer.lemmatize(token).lower()\n",
    "                if word in stopwords: continue\n",
    "                if word in dict_word_freq: dict_word_freq[word] += 1\n",
    "                else: dict_word_freq[word] = 1\n",
    "                \n",
    "    # sort and add first n words in sorted list to vocabulary\n",
    "    sorted_list = sorted(dict_word_freq.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    if n < len(sorted_list): sorted_list = sorted_list[:n]\n",
    "    for word, frequency in sorted_list:\n",
    "        if word not in vocabulary: vocabulary.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fabceea2-794f-4d45-95d2-267d0f432965",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create training data\n",
    "x, y = [], []\n",
    "for i in train_set_x.index:\n",
    "    x.append(get_vector(vocabulary, train_set_x.loc[i, \"text\"]))\n",
    "    y.append(train_set_y.loc[i, \"label\"])\n",
    "\n",
    "# model\n",
    "svm_reg_model = LinearRegression()\n",
    "svm_reg_model.fit(np.asarray(x), np.asarray(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b54d7a07-7d71-435b-bcf2-0b17b9a9880a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = [], []\n",
    "for i in val_set_x.index:\n",
    "    x.append(get_vector(vocabulary, val_set_x.loc[i, \"text\"]))\n",
    "    y.append(val_set_y.loc[i, \"label\"])\n",
    "predictions = svm_reg_model.predict(x)\n",
    "y = np.asarray(y)\n",
    "\n",
    "# transfer regression result to label\n",
    "# TODO: find the appropriate bound for classification\n",
    "predictions = list(predictions)\n",
    "for i in range(len(predictions)):\n",
    "    if predictions[i] < 0.5:\n",
    "        predictions[i] = str(0)\n",
    "    elif predictions[i] < 1.5:\n",
    "        predictions[i] = str(1)\n",
    "    else:\n",
    "        predictions[i] = str(2)\n",
    "predictions = np.asarray(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "362dc154-29e9-4b53-8efd-e5be638d2a6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6615801909032227\n",
      "0.42932723628926156\n",
      "0.39366065989609905\n",
      "0.533\n"
     ]
    }
   ],
   "source": [
    "print(str(precision_score(y, predictions, average='macro')))\n",
    "print(str(recall_score(y, predictions, average='macro')))\n",
    "print(str(f1_score(y, predictions, average='macro')))\n",
    "print(str(accuracy_score(y, predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9b84dc-c422-4f1d-a9c0-5b6a09390d9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eacd9558-c1af-4902-ac29-71fe64011c9e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
