{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7eacb044-d8f8-4806-a1fc-d5c844b9e4ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Nathan\n",
      "[nltk_data]     Hua\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\Nathan\n",
      "[nltk_data]     Hua\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Nathan\n",
      "[nltk_data]     Hua\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to C:\\Users\\Nathan\n",
      "[nltk_data]     Hua\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import operator\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score,accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from matplotlib.pyplot import plot\n",
    "nltk.download('stopwords') # If needed\n",
    "nltk.download('punkt') # If needed\n",
    "nltk.download('wordnet') # If needed\n",
    "nltk.download('omw-1.4') # If needed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1865868b-a1be-4246-821d-80a4a034be23",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8132ee23-6b2b-4a52-926b-0dec208e8112",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label mapping declaration\n",
    "SENTIMENTAL_MAP = {\n",
    "    \"0\": \"negative\",\n",
    "    \"1\": \"neutral\",\n",
    "    \"2\": \"positive\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4c2ffc6c-8cbd-482e-b80e-5e49d74a46a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "for k in SENTIMENTAL_MAP.keys():\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1aae7972-4885-4ede-a024-ce9a76145cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data from txt\n",
    "def read_data(set_name):\n",
    "    text_file_name  = set_name + \"_text.txt\"\n",
    "    label_file_name = set_name + \"_labels.txt\"\n",
    "    text_file = open(\"data/\" + text_file_name, \"r\", encoding=\"utf8\")\n",
    "    label_file = open(\"data/\" + label_file_name, \"r\", encoding=\"utf8\")\n",
    "    x = text_file.readlines()\n",
    "    y = label_file.readlines()\n",
    "    for i in range(len(y)): y[i] = y[i][0]\n",
    "    return pd.DataFrame(x, columns=[\"text\"]), pd.DataFrame(y, columns=[\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "89374b52-8776-4630-897a-0125680ae323",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_x, train_set_y = read_data(\"train\")\n",
    "val_set_x,   val_set_y   = read_data(\"val\")\n",
    "test_set_x,  test_set_y  = read_data(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "edb927e0-6502-49de-af82-9b3107e106e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45615, 1)\n",
      "(45615, 1)\n",
      "(2000, 1)\n",
      "(2000, 1)\n",
      "(12284, 1)\n",
      "(12284, 1)\n"
     ]
    }
   ],
   "source": [
    "print(str(train_set_x.shape))\n",
    "print(str(train_set_y.shape))\n",
    "print(str(val_set_x.shape))\n",
    "print(str(val_set_y.shape))\n",
    "print(str(test_set_x.shape))\n",
    "print(str(test_set_y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0f6f4930-7e86-4601-9dbd-b1c3e3164785",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Omg this show is so predictable even for the 3rd ep. Rui En\\u2019s ex boyfriend was framed for murder probably\\u002c by the rich guy. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(val_set_x.loc[7, \"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e70b08ca-4a2f-44f2-a0fe-d7a79141fd78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ",\n"
     ]
    }
   ],
   "source": [
    "#TODO: remove meaningless text, such as \"@user\" \"#sometagname\"\n",
    "#TODO: remove http links, such as \"https://t.co/4fPkSVlSDl\"\n",
    "#TODO: convert some unicode string to text, such as \"\\u2019\" => \"'\"\n",
    "test = u'\\u002c'\n",
    "print(str(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42029b18-4600-4e07-98bf-e7b8837e15b7",
   "metadata": {},
   "source": [
    "## initial feature and its train, val, test process\n",
    "### TODO: Optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b85853b4-db7a-4a8a-a7f4-aee86894aec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vector(vocab, text):\n",
    "    vector = np.zeros(len(vocab))\n",
    "    words = []\n",
    "    for sentence in nltk.tokenize.sent_tokenize(text):\n",
    "        for token in nltk.tokenize.word_tokenize(sentence):\n",
    "            words.append(lemmatizer.lemmatize(token).lower())\n",
    "    for i, word in enumerate(vocab):\n",
    "        if word in words:\n",
    "            vector[i] = words.count(word)\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d838d07b-4028-4bc6-91dd-f7467f54b5a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial Feature: n most frequent words of each label class and combining them together\n",
    "# define stopwords\n",
    "stopwords = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "additional_stopwords = [\".\", \",\", \"'s\", \"``\", \"''\", \"'\", \"n't\", \"%\", \"-\", \"$\", \"(\", \")\", \":\", \";\", \"@\", \"&\", \"'m\", \"user\", \"#\", \"!\", \"?\", \"...\"]\n",
    "for sw in additional_stopwords: stopwords.add(sw)\n",
    "\n",
    "# Initialize lemmatizer\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "# Get Vocabulary\n",
    "vocabulary = []\n",
    "n = 100\n",
    "for label in SENTIMENTAL_MAP.keys():\n",
    "    # get texts with same label\n",
    "    temp_list = []\n",
    "    for i in train_set_x.index:\n",
    "        if train_set_y.loc[i, \"label\"] == label:\n",
    "            temp_list.append(train_set_x.loc[i, \"text\"])\n",
    "    \n",
    "    # get n most frequent words of this label class\n",
    "    dict_word_freq = {}\n",
    "    for text in temp_list:\n",
    "        for sentence in nltk.tokenize.sent_tokenize(text):\n",
    "            for token in nltk.tokenize.word_tokenize(sentence):\n",
    "                word = lemmatizer.lemmatize(token).lower()\n",
    "                if word in stopwords: continue\n",
    "                if word in dict_word_freq: dict_word_freq[word] += 1\n",
    "                else: dict_word_freq[word] = 1\n",
    "                \n",
    "    # sort and add first n words in sorted list to vocabulary\n",
    "    sorted_list = sorted(dict_word_freq.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    if n < len(sorted_list): sorted_list = sorted_list[:n]\n",
    "    for word, frequency in sorted_list:\n",
    "        if word not in vocabulary: vocabulary.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f4ebfa73-83e6-4ae1-a5e0-b132f88ad653",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training data\n",
    "x, y = [], []\n",
    "for i in train_set_x.index:\n",
    "    x.append(get_vector(vocabulary, train_set_x.loc[i, \"text\"]))\n",
    "    y.append(train_set_y.loc[i, \"label\"])\n",
    "\n",
    "# Init and train model\n",
    "svm_clf_category = sklearn.svm.SVC(kernel=\"linear\", gamma='auto')\n",
    "svm_clf_category.fit(np.asarray(x), np.asarray(y))\n",
    "\n",
    "# test with val set\n",
    "x, y = [], []\n",
    "for i in val_set_x.index:\n",
    "    x.append(get_vector(vocabulary, val_set_x.loc[i, \"text\"]))\n",
    "    y.append(val_set_y.loc[i, \"label\"])\n",
    "predictions = svm_clf_category.predict(x)\n",
    "y = np.asarray(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7ce4e1d4-9dbc-46ce-ab95-359a4164fa55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5940554093980991\n",
      "0.4559994623285762\n",
      "0.44029333218226435\n",
      "0.547\n"
     ]
    }
   ],
   "source": [
    "print(str(precision_score(y, predictions, average='macro')))\n",
    "print(str(recall_score(y, predictions, average='macro')))\n",
    "print(str(f1_score(y, predictions, average='macro')))\n",
    "print(str(accuracy_score(y, predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a15b22f-815a-4e2f-8d65-bd9cb1fdde1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bf0fcda7-7f7b-4b16-8d91-a57159895f08",
   "metadata": {},
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "442f0421-1635-4545-aef2-fdc61dcc3ecc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6687085346069246\n",
      "0.6222323738146524\n",
      "0.6365109146172074\n",
      "0.674\n"
     ]
    }
   ],
   "source": [
    "# Initialize Tfidf Vectorizer\n",
    "tfidf_vector = TfidfVectorizer()\n",
    "# Learn vocabulary and idf from training set\n",
    "tfidf_vector.fit(train_set_x[\"text\"])\n",
    "# Transform train and test input documents to document-term matrix\n",
    "tfidf_train_x = tfidf_vector.transform(train_set_x[\"text\"])\n",
    "tfidf_val_x  = tfidf_vector.transform(val_set_x[\"text\"])\n",
    "\n",
    "# Train the classifier\n",
    "svm_clf_category.fit(tfidf_train_x, train_set_y.iloc[:,-1].to_numpy())\n",
    "# Test with test data\n",
    "predictions = svm_clf_category.predict(tfidf_val_x)\n",
    "tfidf_val_y = val_set_y.to_numpy()\n",
    "    \n",
    "print(str(precision_score(tfidf_val_y, predictions, average='macro')))\n",
    "print(str(recall_score(tfidf_val_y, predictions, average='macro')))\n",
    "print(str(f1_score(tfidf_val_y, predictions, average='macro')))\n",
    "print(str(accuracy_score(tfidf_val_y, predictions)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "23859801-9b2f-4f45-8400-46b6c1a82c6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.598364428981503\n",
      "0.5654939929988184\n",
      "0.5680575073372092\n",
      "0.5944317811787692\n"
     ]
    }
   ],
   "source": [
    "tfidf_test_x  = tfidf_vector.transform(test_set_x[\"text\"])\n",
    "predictions = svm_clf_category.predict(tfidf_test_x)\n",
    "tfidf_test_y = test_set_y.to_numpy()\n",
    "print(str(precision_score(tfidf_test_y, predictions, average='macro')))\n",
    "print(str(recall_score(tfidf_test_y, predictions, average='macro')))\n",
    "print(str(f1_score(tfidf_test_y, predictions, average='macro')))\n",
    "print(str(accuracy_score(tfidf_test_y, predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "032e152b-6770-4d6d-b7bd-03aa3e0a9de8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
